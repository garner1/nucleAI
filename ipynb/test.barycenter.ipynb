{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "import sys  \n",
    "\n",
    "sys.path.insert(0, '../py')\n",
    "from graviti import *\n",
    "\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import timeit\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#frequency = int(sys.argv[1]) # how often to pick a nuclei as a seed = size of the covd sample nuclei\n",
    "#dirpath = sys.argv[2] # the full path to the sample directory\n",
    "\n",
    "frequency = 10 # how often to pick a nuclei as a seed = size of the covd sample nuclei\n",
    "dirpath = '/home/garner1/Work/dataset/tcga_polygons/LUAD/TCGA-75-5146-01Z-00-DX1.4958A631-7E6F-4FBB-A1C3-B8F8368D46C5.svs.tar.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = os.path.basename(dirpath).split(sep='.')[0]; print(sample)\n",
    "\n",
    "print('Loading the data')\n",
    "df = pd.DataFrame()\n",
    "fovs = glob.glob(dirpath+'/*_polygon/*.svs/*.csv.morphometrics.pkl')\n",
    "\n",
    "print('There are '+str(len(fovs))+' FOVs')\n",
    "for fov in fovs: # for each fov\n",
    "    data = pd.read_pickle(fov)\n",
    "    df = df.append(data, ignore_index = True)\n",
    "\n",
    "df['area'] = df['area'].astype(float) # convert to float this field\n",
    "df['circularity'] = 4.0*np.pi*df['area'] / (df['perimeter']*df['perimeter']) # add circularity\n",
    "\n",
    "df = df.head(n=10000) # hard-coded downsize for memory issues \n",
    "\n",
    "numb_nuclei = df.shape[0] \n",
    "print(str(numb_nuclei)+' nuclei')\n",
    "\n",
    "size = numb_nuclei//frequency\n",
    "fdf = df.sample(n=size,random_state=1234) #!!!hard-coded random state \n",
    "print('We consider '+str(size)+' descriptors')\n",
    "\n",
    "centroids = df.columns[:2];# print(centroids)\n",
    "X = df[centroids].to_numpy() # the full array of position\n",
    "n_neighbors = frequency + 10 # the number of nuclei in each descriptor\n",
    "print('Characterizing the neighborhood')\n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree',n_jobs=-1).fit(X) \n",
    "distances, indices = nbrs.kneighbors(X) \n",
    "\n",
    "# Parallel generation of the local covd\n",
    "data = df.to_numpy()\n",
    "print('Generating the descriptor')\n",
    "num_cores = multiprocessing.cpu_count() # numb of cores\n",
    "node_vec_switch_centroid = Parallel(n_jobs=num_cores)(\n",
    "    delayed(covd_parallel_sparse)(node,data,indices) for node in tqdm(list(fdf.index))\n",
    "    )\n",
    "\n",
    "nodes_with_covd = [l[0] for l in node_vec_switch_centroid if l[2] == 1] # list of nodes with proper covd\n",
    "nodes_wo_covd = [l[0] for l in node_vec_switch_centroid if l[2] == 0] # list of nodes wo covd\n",
    "fdf['covd'] = [0 for i in range(fdf.shape[0])]\n",
    "fdf.loc[nodes_with_covd,'covd'] = 1 # identify nodes with covd in dataframe\n",
    "fdf.loc[nodes_wo_covd,'covd'] = 0 # identify nodes wo covd in dataframe    \n",
    "print('There are '+str(len(nodes_with_covd))+' nodes with covd properly defined')\n",
    "\n",
    "# Add the descriptor feature to fdf\n",
    "fdf[\"descriptor\"] = \"\"; fdf[\"descriptor\"].astype(object)\n",
    "for item in node_vec_switch_centroid:\n",
    "    descriptor = item[1]\n",
    "    centroid = item[3]\n",
    "    node = item[0]\n",
    "    fdf.at[node,'descriptor'] = pd.Series(descriptor).values\n",
    "\n",
    "# Generate the descriptor array\n",
    "descriptor = np.zeros((len(nodes_with_covd),node_vec_switch_centroid[0][1].shape[0]))\n",
    "r_idx = 0\n",
    "for index, row in fdf.iterrows():\n",
    "    if row['covd']:\n",
    "        descriptor[r_idx,:] = row['descriptor']\n",
    "        r_idx += 1\n",
    "\n",
    "mean_covd = np.mean(descriptor,axis=0) # evaluate the barycenter descriptor\n",
    "\n",
    "delta = descriptor-mean_covd # evaluate the distance vec of the barycenter from each descriptor\n",
    "distance_from_barycenter = norm(delta,axis=1) # take the eucledean norm\n",
    "\n",
    "# Update the dataframe\n",
    "fdf.loc[nodes_with_covd,'heterogeneity'] = distance_from_barycenter\n",
    "fdf.loc[nodes_wo_covd,'heterogeneity'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = dirpath+'/'+sample+'.nuclei'+str(numb_nuclei)+'.size'+str(size)+'.covdNN'+str(n_neighbors)+'.features.pkl'\n",
    "fdf.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show contour plot\n",
    "N = 10\n",
    "filename = 'test.sum'\n",
    "contourPlot(fdf[(fdf['covd']==True) & (fdf['heterogeneity']>0)],N,np.sum,filename)\n",
    " # Show the distribution\n",
    "fdf['heterogeneity'].hist(bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
