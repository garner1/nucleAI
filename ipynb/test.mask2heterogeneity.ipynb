{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys  \n",
    "sys.path.insert(0, '../py')\n",
    "from graviti import *\n",
    "\n",
    "from numpy.linalg import norm\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "from os import path\n",
    "import sys\n",
    "import glob\n",
    "import h5py\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg')\n",
    "import plotly.graph_objects as go\n",
    "from plotly.graph_objs import *\n",
    "import plotly.express as px\n",
    "import hdbscan\n",
    "import pandas as pd\n",
    "import umap\n",
    "import networkx as nx\n",
    "from scipy import sparse, linalg\n",
    "import pickle\n",
    "from sklearn.preprocessing import normalize, scale\n",
    "from scipy.sparse import find\n",
    "from numpy.linalg import norm\n",
    "import timeit\n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 100000 # number of nuclei to sample, use 0 value for full set\n",
    "nn = 10 # set the number of nearest neighbor in the umap-graph. Will be used in CovD as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = glob.glob('../data/TCGA*.gz')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numb of cores\n",
    "num_cores = multiprocessing.cpu_count() # numb of cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-MP-A4TJ-01Z-00-DX1\n",
      "Loading the data\n",
      "1430788 nuclei\n"
     ]
    }
   ],
   "source": [
    "dirpath = '../data/TCGA-MP-A4TJ-01Z-00-DX1.14EDBE5C-5D0C-4002-BE95-AF5C9D9F3D43.svs.tar.gz'\n",
    "sample = os.path.basename(dirpath).split(sep='.')[0]; print(sample)\n",
    "\n",
    "print('Loading the data')\n",
    "df = pd.DataFrame()\n",
    "fovs = glob.glob(dirpath+'/*_polygon/*.svs/*.pkl')\n",
    "for fov in fovs: # for each fov\n",
    "    data = pd.read_pickle(fov)\n",
    "    df = df.append(data, ignore_index = True)\n",
    "\n",
    "df['area'] = df['area'].astype(float) # convert to float this field\n",
    "\n",
    "#df = df.head(n=10000) # consider smaller df\n",
    "    \n",
    "print(str(df.shape[0])+' nuclei')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downsampling 100000 nuclei\n",
      "Creating the UMAP graph\n",
      "Characterizing the neighborhood\n"
     ]
    }
   ],
   "source": [
    "centroids = df.columns[:2];# print(centroids)\n",
    "\n",
    "if size == 0:\n",
    "    print('Considering all nuclei')\n",
    "    fdf = df \n",
    "else:\n",
    "    print('Downsampling '+str(size)+' nuclei')\n",
    "    fdf = df.sample(n=size) \n",
    "\n",
    "print('Creating the UMAP graph')\n",
    "pos = fdf[centroids].to_numpy() # Get the positions of centroids \n",
    "A = space2graph(pos,nn)\n",
    "print('Characterizing the neighborhood')\n",
    "X = df[centroids].to_numpy() # the full array of position\n",
    "if size is not 0:\n",
    "    n_neighbors = df.shape[0]//size + 10\n",
    "else:\n",
    "    n_neighbors = 10    \n",
    "nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree',n_jobs=-1).fit(X) \n",
    "distances, indices = nbrs.kneighbors(X) \n",
    "\n",
    "#get the morphological data and rescale the data by std\n",
    "df['circularity'] = 4.0*np.pi*df['area'] / (df['perimeter']*df['perimeter'])\n",
    "df['area_rescaled'] = df['area'] / df['area'].mean()\n",
    "df['perimeter_rescaled'] = df['perimeter'] / df['perimeter'].mean()\n",
    "features = ['area_rescaled', 'eccentricity', 'orientation','perimeter_rescaled', 'solidity','circularity']\n",
    "\n",
    "data = df[features].to_numpy(); #print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the descriptor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 144/100000 [01:39<19:12:04,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "# Parallel generation of the local covd\n",
    "print('Generating the descriptor')\n",
    "processed_list = Parallel(n_jobs=num_cores)(\n",
    "    delayed(covd_parallel_sparse)(node,data,indices) for node in tqdm(list(fdf.index))\n",
    ")\n",
    "# store the descriptors\n",
    "filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.descriptors.pkl'\n",
    "pickle.dump( processed_list, open( filename, \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "nodes_with_covd = [l[0] for l in processed_list if l[2] == 0] # list of nodes with proper covd\n",
    "nodes_wo_covd = [l[0] for l in processed_list if l[2] == 1] # list of nodes wo covd\n",
    "fdf['covd'] = [0 for i in range(fdf.shape[0])]\n",
    "fdf.loc[nodes_wo_covd,'covd'] = 0 # identify nodes wo covd in dataframe\n",
    "fdf.loc[nodes_with_covd,'covd'] = 1 # identify nodes with covd in dataframe\n",
    "    \n",
    "print('There are '+str(len(nodes_with_covd))+' nodes with covd properly defined')\n",
    "\n",
    "# Construct the descriptor array\n",
    "descriptor = np.zeros((len(processed_list),processed_list[0][1].shape[0]),dtype=complex)\n",
    "for r in range(len(processed_list)):\n",
    "    descriptor[r,:] = processed_list[r][1] # descriptors of the nuclei communities seeded at sampled nodes\n",
    "\n",
    "# Get positions in fdf.index of nodes_with_covd\n",
    "fdf2adj = {value: counter for (counter, value) in enumerate(fdf.index)}\n",
    "adj2fdf = {v: k for k, v in fdf2adj.items()}\n",
    "\n",
    "# Get info about the graph\n",
    "A[[fdf2adj[n] for n in nodes_wo_covd]] = 0 # zero-out nodes with no proper covd \n",
    "A[:,[fdf2adj[n] for n in nodes_wo_covd]] = 0 # zero-out nodes with no proper covd\n",
    "row_idx, col_idx, values = find(A) #A.nonzero() # nonzero entries\n",
    "\n",
    "print('Generating the heterogeneity metric')\n",
    "node_nn_heterogeneity_weights = Parallel(n_jobs=num_cores)(\n",
    "    delayed(covd_gradient_parallel)(fdf2adj[node],descriptor,row_idx,col_idx,values) \n",
    "    for node in tqdm(nodes_with_covd)\n",
    ")\n",
    "    \n",
    "# define and store dataframe with pairwise diversities\n",
    "heterogeneity_df = pd.DataFrame(node_nn_heterogeneity_weights, columns =['node', 'nn', 'heterogeneity', 'weight']) \n",
    "filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.pairwise_heterogeneity.pkl'\n",
    "heterogeneity_df.to_pickle(filename)\n",
    "\n",
    "fdf['heterogeneity'] = np.nan # create a new feature in fdf\n",
    "for idx in list(fdf.index):\n",
    "    try:\n",
    "        fdf.at[idx,'heterogeneity'] = np.sum(heterogeneity_df[heterogeneity_df['node']==fdf2adj[idx]]['heterogeneity'].values[0])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# store the node diversity dataframe\n",
    "filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.nodeHI.pkl'\n",
    "fdf.to_pickle(filename)\n",
    "\n",
    "print('Generating the edge diversity index and its coordinates')\n",
    "edges_list = Parallel(n_jobs=num_cores)(\n",
    "     delayed(edge_diversity_parallel)(adj2fdf[node],[adj2fdf[nn] for nn in neightbors],diversity,fdf) \n",
    "     for (node, neightbors, diversity, weights) in tqdm(node_nn_heterogeneity_weights) if adj2fdf[node] in nodes_with_covd\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-MP-A4TJ-01Z-00-DX1\n",
      "Loading the data\n",
      "10000 nuclei\n",
      "Downsampling 1000 nuclei\n",
      "Creating the UMAP graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characterizing the neighborhood\n",
      "Generating the descriptor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:13<00:00, 75.80it/s]\n",
      "  0%|          | 0/999 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 999 nodes with covd properly defined\n",
      "Generating the heterogeneity metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999/999 [00:11<00:00, 87.64it/s]\n",
      "100%|██████████| 999/999 [00:00<00:00, 92535.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the edge diversity index and its coordinates\n",
      "4372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "positional indexers are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2129\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3604\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3605\u001b[0m         )\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b1446f6a9d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlista\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;31m# edges_list = Parallel(n_jobs=num_cores)(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;31m#     delayed(edge_diversity_parallel)(adj2fdf[node],[adj2fdf[nn] for nn in neightbors],diversity,fdf)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2146\u001b[0m         \u001b[0;31m# a list of integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_list_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2150\u001b[0m         \u001b[0;31m# a single integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_list_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2131\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m             \u001b[0;31m# re-raise with different error message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2133\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"positional indexers are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: positional indexers are out-of-bounds"
     ]
    }
   ],
   "source": [
    "# edge_list = [item for sublist in edges_list for item in sublist]\n",
    "# edge_df = pd.DataFrame(edge_list, columns=[\"centroid_x\", \"centroid_y\",\"heterogeneity\"]) \n",
    "    \n",
    "# # store the edge diversity dataframe\n",
    "# filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.edgeHI.pkl'\n",
    "# edge_df.to_pickle(filename)\n",
    "\n",
    "# #Show contour plot\n",
    "# N = 10\n",
    "# filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.contour.node.mean.png'\n",
    "# contourPlot(fdf,N,np.mean,filename)\n",
    "\n",
    "# #Show contour plot\n",
    "# N = 10\n",
    "# filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.contour.edge.mean.png'\n",
    "# contourPlot(edge_df,N,np.mean,filename)\n",
    "\n",
    "# #Show contour plot\n",
    "# N = 10\n",
    "# filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.contour.node.sum.png'\n",
    "# contourPlot(fdf,N,np.sum,filename)\n",
    "\n",
    "# #Show contour plot\n",
    "# N = 10\n",
    "# filename = dirpath+'/'+sample+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.contour.edge.sum.png'\n",
    "# contourPlot(edge_df,N,np.sum,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCGA-MP-A4TJ-01Z-00-DX1\n",
      "Loading the data\n",
      "9492 nuclei\n",
      "Considering all nuclei\n",
      "Creating the UMAP graph\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9492 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characterizing the neighborhood\n",
      "Generating the descriptor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9492/9492 [00:19<00:00, 489.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 9480 nodes with covd properly defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/9480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the heterogeneity metric\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9480/9480 [00:07<00:00, 1245.68it/s]\n",
      "  0%|          | 0/9480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating the edge diversity index and its coordinates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9480/9480 [00:20<00:00, 467.35it/s]\n"
     ]
    }
   ],
   "source": [
    "for dirpath in samples[:1]: # for each sample\n",
    "    sample = os.path.basename(dirpath).split(sep='.')[0]; print(sample)\n",
    "\n",
    "    print('Loading the data')\n",
    "    df = pd.DataFrame()\n",
    "    fovs = glob.glob(dirpath+'/*_polygon/*.svs/*.pkl')\n",
    "    for fov in fovs[:2]: # for each fov\n",
    "        data = pd.read_pickle(fov)\n",
    "        df = df.append(data, ignore_index = True)\n",
    "        #print(df.shape)\n",
    "\n",
    "    df['area'] = df['area'].astype(float) # convert to float this field\n",
    "    \n",
    "    df = df.head(n=100000) # filter first x nuclei\n",
    "    \n",
    "    print(str(df.shape[0])+' nuclei')\n",
    "    \n",
    "    centroids = df.columns[:2];# print(centroids)\n",
    "\n",
    "    if size == 0:\n",
    "        print('Considering all nuclei')\n",
    "        fdf = df \n",
    "    else:\n",
    "        print('Downsampling '+str(size)+' nuclei')\n",
    "        fdf = df.sample(n=size) \n",
    "    pos = fdf[centroids].to_numpy() # Get the positions of centroids \n",
    "    \n",
    "    print('Creating the UMAP graph')\n",
    "    A = space2graph(pos,nn)\n",
    "    \n",
    "    print('Characterizing the neighborhood')\n",
    "    X = df[centroids].to_numpy() # the full array of position\n",
    "    if size is not 0:\n",
    "        n_neighbors = df.shape[0]//size + 10\n",
    "    else:\n",
    "        n_neighbors = 10    \n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='kd_tree',n_jobs=-1).fit(X) \n",
    "    distances, indices = nbrs.kneighbors(X) \n",
    "\n",
    "    #get the morphological data and rescale the data by std\n",
    "    df['circularity'] = 4.0*np.pi*df['area'] / (df['perimeter']*df['perimeter'])\n",
    "    df['area_rescaled'] = df['area'] / df['area'].mean()\n",
    "    df['perimeter_rescaled'] = df['perimeter'] / df['perimeter'].mean()\n",
    "    features = ['area_rescaled', 'eccentricity', 'orientation','perimeter_rescaled', 'solidity','circularity']\n",
    "\n",
    "    data = df[features].to_numpy()\n",
    "    \n",
    "    # Parallel generation of the local covd\n",
    "    print('Generating the descriptor')\n",
    "    processed_list = Parallel(n_jobs=num_cores)(\n",
    "        delayed(covd_parallel_sparse)(node,data,indices) for node in tqdm(list(fdf.index))\n",
    "    )\n",
    "    # store the descriptors\n",
    "    filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.descriptors.pkl'\n",
    "    pickle.dump( processed_list, open( filename, \"wb\" ) )\n",
    "    \n",
    "    nodes_with_covd = [l[0] for l in processed_list if l[2] == 0] # list of nodes with proper covd\n",
    "    nodes_wo_covd = [l[0] for l in processed_list if l[2] == 1] # list of nodes wo covd\n",
    "    fdf['covd'] = [0 for i in range(fdf.shape[0])]\n",
    "    fdf.loc[nodes_wo_covd,'covd'] = 0 # identify nodes wo covd in dataframe\n",
    "    fdf.loc[nodes_with_covd,'covd'] = 1 # identify nodes with covd in dataframe\n",
    "    \n",
    "    print('There are '+str(len(nodes_with_covd))+' nodes with covd properly defined')\n",
    "\n",
    "    # Construct the descriptor array\n",
    "    descriptor = np.zeros((len(processed_list),processed_list[0][1].shape[0]),dtype=complex)\n",
    "    for r in range(len(processed_list)):\n",
    "        descriptor[r,:] = processed_list[r][1] # descriptors of the nuclei communities seeded at sampled nodes\n",
    "        \n",
    "    # Get info about the graph\n",
    "    A[nodes_wo_covd] = 0 # zero-out nodes with no proper covd \n",
    "    A[:,nodes_wo_covd] = 0 # zero-out nodes with no proper covd\n",
    "    row_idx, col_idx, values = find(A) #A.nonzero() # nonzero entries\n",
    "    print('Generating the heterogeneity metric')\n",
    "    node_nn_heterogeneity_weights = Parallel(n_jobs=num_cores)(\n",
    "        delayed(covd_gradient_parallel)(node,descriptor,row_idx,col_idx,values) \n",
    "        for node in tqdm(nodes_with_covd))\n",
    "    \n",
    "    # define and store dataframe with pairwise diversities\n",
    "    heterogeneity_df = pd.DataFrame(node_nn_heterogeneity_weights, columns =['node', 'nn', 'heterogeneity', 'weight']) \n",
    "    filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.pairwise_heterogeneity.pkl'\n",
    "    heterogeneity_df.to_pickle(filename)\n",
    "    \n",
    "    fdf['heterogeneity'] = np.nan # create a new feature in fdf\n",
    "    for idx in list(fdf.index):\n",
    "        try:\n",
    "            fdf.at[idx,'heterogeneity'] = np.sum(heterogeneity_df[heterogeneity_df['node']==idx]['heterogeneity'].values[0])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    # store the node diversity dataframe\n",
    "    filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.nodeHI.pkl'\n",
    "    fdf.to_pickle(filename)\n",
    "\n",
    "    print('Generating the edge diversity index and its coordinates')\n",
    "    edges_list = Parallel(n_jobs=num_cores)(\n",
    "        delayed(edge_diversity_parallel)(node,neightbors,diversity,fdf) \n",
    "                               for (node, neightbors, diversity, weights) in tqdm(node_nn_heterogeneity_weights) if node in nodes_with_covd\n",
    "    )\n",
    "    edge_list = [item for sublist in edges_list for item in sublist]\n",
    "    edge_df = pd.DataFrame(edge_list, columns=[\"centroid_x\", \"centroid_y\",\"heterogeneity\"]) \n",
    "    \n",
    "    # store the edge diversity dataframe\n",
    "    filename = './'+str(sample)+'.size'+str(size)+'.graphNN'+str(nn)+'.covdNN'+str(n_neighbors)+'.edgeHI.pkl'\n",
    "    edge_df.to_pickle(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show contour plot\n",
    "N = 200\n",
    "filename = 'test'\n",
    "contourPlot(fdf,N,np.mean,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show contour plot\n",
    "N = 200\n",
    "filename = 'test'\n",
    "contourPlot(edge_df,N,np.mean,filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
